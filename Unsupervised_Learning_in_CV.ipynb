{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1EbiBSLOj7scfaN42YoFm-jxuTIHTfbk_",
      "authorship_tag": "ABX9TyPlLAC2meVFEIZ0W3MrTZ0F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dnzambuli/Deep-learning-4-CV/blob/master/Unsupervised_Learning_in_CV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Objective\n",
        "This assignment aims to assess your ability to apply unsupervised learning techniques in computer vision. You will work with an image dataset and implement different methods to extract meaningful representations, analyze patterns, and evaluate model performance.\n",
        "\n",
        "## Expected Learning Outcomes\n",
        "By completing this assignment, you will:\n",
        "\n",
        "✅ Understand the application of unsupervised learning in computer vision.\n",
        "\n",
        "✅ Develop hands-on experience in clustering, dimensionality reduction, and contrastive learning.\n",
        "\n",
        "✅ Analyze and compare different representation learning techniques.\n",
        "\n",
        "✅ Enhance your ability to work with image datasets in machine learning pipelines.\n",
        "\n",
        "# Instructions\n",
        "* Use **Python** and frameworks such as **NumPy**, **scikit-learn**, **PyTorch**, and **OpenCV**.\n",
        "\n",
        "* Write clean, well-commented code.\n",
        "* Submit a **Jupyter Notebook/Python script** with a short report explaining your methodology, results, and observations.\n",
        "* Include visualizations wherever necessary."
      ],
      "metadata": {
        "id": "83OFB0h_mQLy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1_7rYLHmAmP",
        "outputId": "8e9078fe-fc4d-4378-f65e-4d38027781c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# mount google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read and extract to the local session the content from /content/drive/MyDrive/DL4CV\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "import zipfile\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the source directory containing the zip files\n",
        "source_dir = '/content/drive/MyDrive/DL4CV'\n",
        "\n",
        "# Define the destination directory to extract the zip files\n",
        "destination_dir = '/content/'\n",
        "if not os.path.exists(destination_dir):\n",
        "  os.makedirs(destination_dir)\n",
        "\n",
        "# Iterate through the files in the source directory\n",
        "for filename in os.listdir(source_dir):\n",
        "  source_file = os.path.join(source_dir, filename)\n",
        "\n",
        "  # Check if the file is a zip file\n",
        "  if filename.endswith('.zip'):\n",
        "    print(f\"Extracting {filename}...\")\n",
        "\n",
        "    # Extract the zip file to the destination directory\n",
        "    with zipfile.ZipFile(source_file, 'r') as zip_ref:\n",
        "      zip_ref.extractall(destination_dir)\n",
        "\n",
        "    print(f\"Extraction completed for {filename}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HW6OWWSYoqEj",
        "outputId": "b1d1300f-09dd-451f-ace6-495bda1735e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Extracting Road Crack Detection-Combined Dataset.zip...\n",
            "Extraction completed for Road Crack Detection-Combined Dataset.zip.\n",
            "Extracting Road cracks.zip...\n",
            "Extraction completed for Road cracks.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# COUNT THE FILES IN ROAD CRACKS FOLDER AND TRAIN FOLDER\n",
        "def count_files_in_folder(folder_path):\n",
        "  \"\"\"Counts the number of files in a given folder.\n",
        "\n",
        "  Args:\n",
        "    folder_path: The path to the folder.\n",
        "\n",
        "  Returns:\n",
        "    The number of files in the folder.\n",
        "  \"\"\"\n",
        "  if not os.path.exists(folder_path):\n",
        "    return 0\n",
        "  count = 0\n",
        "  for _, _, files in os.walk(folder_path):\n",
        "    count += len(files)\n",
        "  return count\n",
        "\n",
        "\n",
        "road_cracks_folder = '/content/Road cracks'\n",
        "train_folder = '/content/train'\n",
        "\n",
        "num_road_cracks_files = count_files_in_folder(road_cracks_folder)\n",
        "num_train_files = count_files_in_folder(train_folder)\n",
        "\n",
        "print(f\"Number of files in road_cracks folder: {num_road_cracks_files}\")\n",
        "print(f\"Number of files in train folder: {num_train_files}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2NVZ1OO13Zo",
        "outputId": "4b3262dc-9882-4f18-d835-d2cba403ea96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of files in road_cracks folder: 125\n",
            "Number of files in train folder: 1389\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check which files are in Road cracks and repeated in train\n",
        "\n",
        "def find_common_files(folder1, folder2):\n",
        "  \"\"\"Finds files that are present in both folders.\n",
        "\n",
        "  Args:\n",
        "    folder1: The path to the first folder.\n",
        "    folder2: The path to the second folder.\n",
        "\n",
        "  Returns:\n",
        "    A list of file names that are present in both folders.\n",
        "  \"\"\"\n",
        "  files1 = set(os.listdir(folder1))\n",
        "  files2 = set(os.listdir(folder2))\n",
        "  common_files = files1.intersection(files2)\n",
        "  return list(common_files)\n",
        "\n",
        "\n",
        "road_cracks_folder = '/content/train'\n",
        "train_folder = '/content/Road cracks'\n",
        "\n",
        "common_files = find_common_files(road_cracks_folder, train_folder)\n",
        "\n",
        "print(\"Files present in both 'Road cracks' and 'train' folders:\")\n",
        "for file in common_files:\n",
        "  print(file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9E4NvkB2YE4",
        "outputId": "ab9340f8-5257-44f2-efca-f700b0a07a2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files present in both 'Road cracks' and 'train' folders:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1: Image Data Preprocessing\n",
        "\n",
        "Perform necessary data preprocessing:\n",
        "1. Convert images to grayscale (if required).\n",
        "2. Normalize pixel values.\n",
        "3. Apply data augmentation techniques (e.g., flipping, rotation)\n",
        "\n",
        "## Deliverables:\n",
        "* Code implementation\n",
        "* Short explanation of preprocessing steps"
      ],
      "metadata": {
        "id": "7nywZCiepTEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import random"
      ],
      "metadata": {
        "id": "vLZH4BUFpU7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing Implementation\n",
        "## 1. Loading and Normalizing Images\n",
        "### Steps:\n",
        "1. **Extract Images and Filenames**\n",
        "- Load images from their respective directories along with their filenames.\n",
        "\n",
        "> *Why?*\n",
        ">\n",
        ">This ensures proper tracking of images and their labels (if filenames contain metadata).\n",
        "\n",
        "2. **Convert Images to Grayscale**\n",
        "\n",
        "- Transform RGB/BGR images into single-channel grayscale.\n",
        "\n",
        "> *Why?*\n",
        ">\n",
        "> Reduces computational complexity (1 channel instead of 3).\n",
        ">\n",
        "> Since crack detection relies on structural patterns rather than color, grayscale preserves necessary features while simplifying the model.\n",
        "\n",
        "3. **Resize Images to (128×128) and Normalize**\n",
        "\n",
        "- Resize all images to a fixed dimension (128×128).\n",
        "\n",
        "- Normalize pixel values (e.g., scale to [0, 1] or standardize using mean/std).\n",
        "\n",
        "> *Why?*\n",
        ">\n",
        ">Ensures uniform input size for neural networks.\n",
        ">\n",
        "> Normalization helps models converge faster by keeping gradients stable.\n",
        "\n",
        "4. Add Channel Dimension for Grayscale\n",
        "\n",
        "- Reshape images from (128, 128) to (128, 128, 1).\n",
        "\n",
        "> *Why?*\n",
        ">\n",
        "> Deep learning frameworks (TensorFlow/PyTorch) expect explicit channel dimensions, even for grayscale.\n",
        "\n",
        "5. **Return Images with Filenames**\n",
        "\n",
        "- Maintain a mapping between processed images and their original filenames for traceability.\n",
        "\n",
        "## 2. Data Augmentation\n",
        "### Steps:\n",
        "1. **Apply Random Transformations**\n",
        "\n",
        "- Random horizontal/vertical flips.\n",
        "\n",
        "- Rotations (90°, 180°, 270°).\n",
        "\n",
        "> *Why?*\n",
        ">\n",
        "> Increases dataset diversity artificially.\n",
        ">\n",
        "> Helps prevent overfitting by exposing the model to more variations (e.g., cracks at different orientations).\n",
        "\n",
        "2. **Combine Augmented Data into Unified Arrays**\n",
        "\n",
        "- Merge original and augmented images into a single NumPy array (X).\n",
        "\n",
        "- Store corresponding labels in another array (y).\n",
        "\n",
        "> *Why?*\n",
        ">\n",
        "> Streamlines training by providing a single, shuffled dataset.\n",
        ">\n",
        "\n",
        "## 3. Convert Images to .keras Format\n",
        "\n",
        "> Why?\n",
        ">\n",
        "> **Efficiency:** .keras (or .tfrecord) is optimized for TensorFlow/PyTorch, enabling faster I/O during training.\n",
        ">\n",
        "> **Portability:** Serializes data + metadata (labels, normalization stats) in a standardized format.\n",
        ">\n",
        "> **Reproducibility:** Ensures consistent preprocessing across experiments.\n",
        "\n",
        "## 4. Data Splitting\n",
        "### Steps:\n",
        "> - Split `content/train` (1,389 images) into 80% Train / 20% Validation\n",
        ">\n",
        "> *Why?*\n",
        ">\n",
        "> **Training set:** Used to train the model.\n",
        ">\n",
        "> **Validation set:** Monitors model performance during training (e.g., tuning hyperparameters, detecting overfitting).\n",
        "\n",
        "\n",
        "\n",
        "> - Use `content/Road Cracks` (125 images) as Test Set\n",
        ">\n",
        "> *Why?*\n",
        ">\n",
        "> Provides an unseen dataset to evaluate final model generalization.\n",
        ">\n",
        "> Ensures unbiased performance metrics (avoids data leakage from train/val sets)."
      ],
      "metadata": {
        "id": "5PypKiXnpEv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import random\n",
        "\n",
        "def load_and_normalize_images(folder_path = \"content/train\", image_size = (128, 128)):\n",
        "  \"\"\"\n",
        "  Loads .jpg images from the folder\n",
        "    converts some to greyscale images\n",
        "  resizes the images, normalizes the pixels to [0,1]\n",
        "\n",
        "  returns:\n",
        "    images (numpy array): Array of preprocessed images\n",
        "\n",
        "    filenames: lists the corresponding filenames\n",
        "  \"\"\"\n",
        "  images = []\n",
        "  filenames = []\n",
        "\n",
        "  for filename in os.listdir(folder_path):\n",
        "    if filename.endswith(\".jpg\"):\n",
        "      img_path = os.path.join(folder_path, filename)\n",
        "\n",
        "      img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE) #convert to greyscale\n",
        "      img = cv2.resize(img, image_size)  # Resize to fixed size\n",
        "      img = img /255.0 # normalize to [0,1]\n",
        "\n",
        "      images.append(img)\n",
        "\n",
        "      filenames.append(filename)\n",
        "\n",
        "  images = np.array(images).astype(\"float32\") # get a np arrray object\n",
        "  images = np.expand_dims(images, axis = -1) # add channel information\n",
        "\n",
        "  return images, filenames\n",
        "\n",
        "def augment_image(image, filename):\n",
        "  \"\"\"\n",
        "  applies data augmentation techniques (flipping, rotating) on images\n",
        "\n",
        "  args:\n",
        "    images (np array): a single greyscale image\n",
        "    filename (str): The original filename.\n",
        "\n",
        "  returns:\n",
        "    augmented_images (list): list of augmented images\n",
        "    augmented_labels (list): Corresponding labels.\n",
        "  \"\"\"\n",
        "  augmented_images = [image] # the original image is added to begin the list\n",
        "  augmented_labels = [filename] # the image lables\n",
        "\n",
        "  # horizontal flip\n",
        "  if random.choice([True, False]):\n",
        "    augmented_images.append(cv2.flip(image, 1))\n",
        "    augmented_labels.append(filename + \"_flipH\")\n",
        "\n",
        "  # vertical flip\n",
        "  if random.choice([True, False]):\n",
        "    augmented_images.append(cv2.flip(image, 0))\n",
        "    augmented_labels.append(filename + \"_flipV\")\n",
        "\n",
        "  # random rotation\n",
        "  rotation_choice = random.choice([0, 90, 180, 270])\n",
        "  if rotation_choice == 90:\n",
        "        augmented_images.append(cv2.rotate(image, cv2.ROTATE_90_CLOCKWISE))\n",
        "        augmented_labels.append(filename + \"_rot90\")\n",
        "    elif rotation_choice == 180:\n",
        "        augmented_images.append(cv2.rotate(image, cv2.ROTATE_180))\n",
        "        augmented_labels.append(filename + \"_rot180\")\n",
        "    elif rotation_choice == 270:\n",
        "        augmented_images.append(cv2.rotate(image, cv2.ROTATE_90_COUNTERCLOCKWISE))\n",
        "        augmented_labels.append(filename + \"_rot270\")\n",
        "\n",
        "  return augmented_images, augmented_labels\n",
        "\n",
        "def augment_dataset(images, filenames):\n",
        "  \"\"\"\n",
        "    Applies random augmentation to the entire dataset.\n",
        "\n",
        "    Args:\n",
        "        images (list of numpy arrays): Original images.\n",
        "        filenames (list): Corresponding filenames.\n",
        "\n",
        "    Returns:\n",
        "        all_images (numpy array): Combined original & augmented images.\n",
        "  \"\"\"\n",
        "  augmented_images = []\n",
        "  augmented_labels = []\n",
        "\n",
        "  for img, fname in zip(images, label):\n",
        "    aug_imgs, aug_labels = augment_image(img, fname)\n",
        "    augmented_images.extend(aug_imgs)\n",
        "    augmented_labels.extend(aug_labels)\n",
        "\n",
        "  return np.array(augmented_images).astype(\"float32\"), np.array(augmented_labels)\n",
        "\n",
        "\n",
        "def convert_to_keras_dataset(images, labels, save_path = \"content/train.keras\"):\n",
        "  \"\"\"\n",
        "  converts a numpy array into a tensorflow object and saves it as a .keras file\n",
        "\n",
        "  Args:\n",
        "    image (numpy array): aArray of processed images\n",
        "    labels (numpy array): Array of labels.\n",
        "    save_path (str): path to save the .keras dataset\n",
        "  \"\"\"\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
        "  tf.data.experimental.save(dataset, save_path)\n",
        "  print(f\"Dataset saved to {save_path}\")\n"
      ],
      "metadata": {
        "id": "CYbi9BsSqvEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images, filenames = load_and_normalize_images()  # Load images\n",
        "augmented_images = augment_dataset(images, filenames)  # Augment dataset\n",
        "\n",
        "# split to 80:20 training:validation dataset\n",
        "train_images, val_images, train_labels, val_labels = train_test_split(\n",
        "        augmented_images, augmented_labels, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "# Save both train and validation datasets\n",
        "convert_to_keras_dataset(train_images, train_labels, \"train_road_cracks.keras\")\n",
        "convert_to_keras_dataset(val_images, val_labels, \"val_road_cracks.keras\")\n",
        "\n",
        "# the test data will be the road_cracks data as no data repeats\n",
        "test_images, test_labels = load_and_normalize_images(folder_path = road_cracks_folder)\n",
        "convert_to_keras_dataset(test_images, test_labels, \"test_road_cracks.keras\")"
      ],
      "metadata": {
        "id": "7rrZi8PynKFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2: Clustering for Image Categorization\n",
        "\n",
        "1. Extract features from images using **Histogram of Oriented Gradients (HOG)**, **ORB**, or\n",
        "**deep features from a pretrained CNN**.\n",
        "2. Apply **K-Means** and **DBSCAN** clustering to group similar images.\n",
        "3. Use the Silhouette Score to evaluate clustering quality.\n",
        "\n",
        "## Deliverables:\n",
        "* Code for feature extraction and clustering\n",
        "* Visualization of clusters (e.g., sample images from each cluster)\n",
        "* Evaluation and interpretation of results"
      ],
      "metadata": {
        "id": "guL8ZPwVrAT8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Extraction & Clustering for Road Crack Detection\n",
        "## 1. Loading the Test Dataset\n",
        "**Input:** Preprocessed .keras file (test_road_cracks.keras).\n",
        "\n",
        "*Why?* Avoids reloading images from folders—data is already normalized and formatted for ML.\n",
        "\n",
        "Example:\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "test_data = tf.keras.models.load_model(\"test_road_cracks.keras\")  # Load serialized dataset\n",
        "images = test_data[\"images\"]  # Extract images\n",
        "labels = test_data[\"labels\"]  # Optional, if available\n",
        "```\n",
        "\n",
        "## 2. Feature Extraction\n",
        "### a. HOG (Histogram of Oriented Gradients)\n",
        "\n",
        "- **What it does:** Captures texture/shape by analyzing gradient directions in image patches.\n",
        "\n",
        "- *Why use it?* Effective for crack detection (cracks have strong edges).\n",
        "\n",
        "- **Example Output:** A feature vector (e.g., `[0.2, 1.7, ...]` length=144).\n",
        "\n",
        "Code Example:\n",
        "\n",
        "```python\n",
        "from skimage.feature import hog\n",
        "hog_features = [hog(img, orientations=9, pixels_per_cell=(8,8)) for img in images]\n",
        "```\n",
        "\n",
        "### b. ORB (Oriented FAST and Rotated BRIEF)\n",
        "- **What it does:** Detects keypoints (corners/blobs) and describes them with binary features.\n",
        "\n",
        "- *Why use it?* Fast and rotation-invariant (good for cracks at different angles).\n",
        "\n",
        "- **Example Output:** Keypoints + descriptors (e.g., `(x,y) coordinates + binary codes`).\n",
        "\n",
        "Code Example:\n",
        "\n",
        "```python\n",
        "import cv2\n",
        "orb = cv2.ORB_create()\n",
        "keypoints, descriptors = orb.detectAndCompute(img, None)  # For each image\n",
        "```\n",
        "\n",
        "### c. Deep Features (MobileNetV2 CNN)\n",
        "- **What it does:** Uses a pre-trained CNN to extract high-level features.\n",
        "\n",
        "- *Why use it?* Captures complex patterns (e.g., subtle cracks) better than handcrafted methods.\n",
        "\n",
        "- **Example Output:** A `1280-dimensional vector` per image.\n",
        "\n",
        "Code Example:\n",
        "\n",
        "```python\n",
        "\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "model = MobileNetV2(weights=\"imagenet\", include_top=False, pooling=\"avg\")\n",
        "deep_features = model.predict(images)\n",
        "```\n",
        "\n",
        "## 3. Clustering Techniques\n",
        "### a. K-Means\n",
        "- **What it does:** Groups images into n_clusters based on feature similarity.\n",
        "\n",
        "- *Why use it?* Simple and fast for well-separated clusters (e.g., `distinct crack types`).\n",
        "\n",
        "> Example:\n",
        ">\n",
        "> Input: HOG features (shape=[num_images, 144]).\n",
        ">\n",
        "> Output: Labels like `[0, 1, 0, 2, ...]` for each cluster.\n",
        ">\n",
        "\n",
        "Code Example:\n",
        "\n",
        "```python\n",
        "from sklearn.cluster import KMeans\n",
        "kmeans = KMeans(n_clusters=3).fit(hog_features)\n",
        "clusters = kmeans.labels_\n",
        "```\n",
        "\n",
        "### b. DBSCAN\n",
        "- **What it does:** Groups images based on density (no fixed cluster count).\n",
        "\n",
        "- *Why use it?* Handles outliers/noise (e.g., `misclassified cracks`).\n",
        "\n",
        "> Example:\n",
        ">\n",
        "> Input: ORB descriptors (after flattening).\n",
        ">\n",
        "> Output: Labels like `[0, -1, 1, ...] (-1 = noise)`.\n",
        "\n",
        "Code Example:\n",
        "\n",
        "```python\n",
        "\n",
        "from sklearn.cluster import DBSCAN\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5).fit(orb_descriptors)\n",
        "```\n",
        "\n",
        "### c. Silhouette Score\n",
        "- **What it does:** Measures how well clusters are separated (range: `[-1, 1]`).\n",
        "\n",
        "- *Why use it?* Evaluates clustering quality (higher = better).\n",
        "\n",
        "Example Output: `0.65 (good)`, `-0.1 (poor)`.\n",
        "\n",
        "Code Example:\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import silhouette_score\n",
        "score = silhouette_score(features, clusters)\n",
        "```\n",
        "## 4. Visualization\n",
        "- **Goal:** Show `5 sample images per cluster` to inspect crack categories.\n",
        "\n",
        "- *Why?* Validates if clusters make sense (e.g., `\"severe cracks\"` vs. `\"hairline cracks\"`).\n",
        "\n",
        "> Example Output:\n",
        ">\n",
        "> Cluster 0: 5 images of wide cracks.\n",
        ">\n",
        "> Cluster 1: 5 images of linear cracks.\n",
        "\n",
        "Code Example:\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "for cluster_id in range(3):\n",
        "    cluster_samples = images[clusters == cluster_id][:5]  # Get 5 samples\n",
        "    plt.figure(figsize=(10, 2))\n",
        "    for i, img in enumerate(cluster_samples):\n",
        "        plt.subplot(1, 5, i+1)\n",
        "        plt.imshow(img, cmap=\"gray\")\n",
        "    plt.title(f\"Cluster {cluster_id}\")\n",
        "```"
      ],
      "metadata": {
        "id": "pt_knYo8M7yG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.metrics import silhouette_score\n",
        "from skimage.feature import hog"
      ],
      "metadata": {
        "id": "Wi4IQMdYTFdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use functions to write DRY code\n",
        "# Load dataset from .keras file\n",
        "def load_keras_dataset(file_path=\"test_road_cracks.keras\"):\n",
        "  '''\n",
        "  Reads the .keras file\n",
        "\n",
        "  Args:\n",
        "    file_path : the path to the test data because its small enough to run\n",
        "    clustering\n",
        "  Output:\n",
        "    images (numpy array): Array of preprocessed images\n",
        "    filenames: lists the corresponding filenames\n",
        "  '''\n",
        "  dataset = tf.data.experimental.load(file_path)\n",
        "\n",
        "  images = []\n",
        "  filenames = []\n",
        "\n",
        "  for img, label in dataset:\n",
        "      images.append(img.numpy())  # Convert Tensor to NumPy\n",
        "      filenames.append(label.numpy().decode(\"utf-8\"))  # Convert Tensor to string\n",
        "\n",
        "  images = np.array(images)\n",
        "  return images, filenames\n",
        "\n",
        "def extract_hog_features(images):\n",
        "    \"\"\"\n",
        "    Extracts HOG features from images.\n",
        "\n",
        "    Args:\n",
        "        images (numpy array): Array of preprocessed images.\n",
        "\n",
        "    Returns:\n",
        "        hog_features (numpy array): Array of HOG features.\n",
        "    \"\"\"\n",
        "    hog_features = []\n",
        "    for img in images:\n",
        "        img = img.squeeze()  # Remove channel dim\n",
        "        feature = hog(img, pixels_per_cell=(8, 8), cells_per_block=(2, 2), feature_vector=True)\n",
        "        hog_features.append(feature)\n",
        "    return np.array(hog_features)\n",
        "\n",
        "def extract_deep_features(images):\n",
        "    \"\"\"\n",
        "    Extracts deep features from images using MobileNetV2.\n",
        "\n",
        "    Args:\n",
        "        images (numpy array): Array of preprocessed images.\n",
        "\n",
        "    Returns:\n",
        "        deep_features (numpy array): Array of deep features.\n",
        "    \"\"\"\n",
        "    model = MobileNetV2(weights=\"imagenet\", include_top=False, pooling=\"avg\", input_shape=(128, 128, 3))\n",
        "    images_rgb = np.stack([cv2.cvtColor(img.squeeze(), cv2.COLOR_GRAY2RGB) for img in images])  # Convert grayscale to RGB\n",
        "    images_preprocessed = preprocess_input(images_rgb)\n",
        "    deep_features = model.predict(images_preprocessed)\n",
        "    return deep_features\n",
        "\n",
        "def extract_orb_features(images):\n",
        "    \"\"\"\n",
        "    Extracts ORB features from images.\n",
        "\n",
        "    Args:\n",
        "        images (numpy array): Array of preprocessed images.\n",
        "\n",
        "    Returns:\n",
        "        orb_features (numpy array): Array of ORB features.\n",
        "    \"\"\"\n",
        "    orb = cv2.ORB_create()\n",
        "    orb_features = []\n",
        "    for img in images:\n",
        "        keypoints, descriptors = orb.detectAndCompute(img, None)\n",
        "        if descriptors is not None:\n",
        "            orb_features.append(descriptors.flatten()[:1000])  # Keep only the first 1000 features\n",
        "        else:\n",
        "            orb_features.append(np.zeros(1000))  # Pad with zeros\n",
        "    return np.array(orb_features)\n",
        "\n",
        "def apply_clustering(features, method=\"kmeans\", eps=0.5, min_samples=5, n_clusters=5):\n",
        "    \"\"\"\n",
        "    Applies clustering to the given features.\n",
        "\n",
        "    Args:\n",
        "        features (numpy array): Array of features.\n",
        "        method (str): Clustering method (\"kmeans\" or \"dbscan\").\n",
        "        eps (float): DBSCAN parameter (for DBSCAN).\n",
        "        min_samples (int): DBSCAN parameter (for DBSCAN).\n",
        "        n_clusters (int): Number of clusters (for KMeans).\n",
        "\n",
        "    Returns:\n",
        "        cluster_labels (numpy array): Array of cluster labels.\n",
        "    \"\"\"\n",
        "    if method == \"kmeans\":\n",
        "        clustering_model = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    elif method == \"dbscan\":\n",
        "        clustering_model = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "\n",
        "    cluster_labels = clustering_model.fit_predict(features)\n",
        "\n",
        "    # Compute silhouette score\n",
        "    silhouette_avg = silhouette_score(features, cluster_labels) if len(set(cluster_labels)) > 1 else -1\n",
        "    print(f\"Silhouette Score for {method}: {silhouette_avg:.4f}\")\n",
        "\n",
        "    return cluster_labels\n",
        "\n",
        "def visualize_clusters(images, labels, filenames, n_clusters=5):\n",
        "    \"\"\"\n",
        "    Visualizes clusters by showing 5 sample images from each cluster.\n",
        "\n",
        "    Args:\n",
        "        images (numpy array): Array of images.\n",
        "        labels (numpy array): Array of cluster labels.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    unique_labels = np.unique(labels)\n",
        "    fig, axes = plt.subplots(len(unique_labels), 5, figsize=(10, len(unique_labels) * 2))\n",
        "\n",
        "    for i, label in enumerate(unique_labels):\n",
        "        indices = np.where(labels == label)[0][:5]  # Pick first 5 images from each cluster\n",
        "        for j, idx in enumerate(indices):\n",
        "            axes[i, j].imshow(images[idx].squeeze(), cmap=\"gray\")\n",
        "            axes[i, j].set_title(f\"Cluster {label}\\n{filenames[idx]}\")\n",
        "            axes[i, j].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ggj3ZLtHraHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. HOG Features"
      ],
      "metadata": {
        "id": "W51QHauwULCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_images, test_filenames = load_keras_dataset(\"test_road_cracks.keras\")\n",
        "hog_features = extract_hog_features(test_images) #_deep_features for deep features\n",
        "#_orb_features for ORB features"
      ],
      "metadata": {
        "id": "mPRMWwMVUBDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Clustering"
      ],
      "metadata": {
        "id": "OnP5TEaUUZDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Clustering\n",
        "kmeans_labels = apply_clustering(hog_features, method=\"kmeans\", n_clusters=5)\n",
        "dbscan_labels = apply_clustering(hog_features, method=\"dbscan\", eps=0.5, min_samples=5)"
      ],
      "metadata": {
        "id": "lKMuoe9nUb9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Visualize"
      ],
      "metadata": {
        "id": "mEPohqHSUknC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize Clusters\n",
        "visualize_clusters(test_images, kmeans_labels, test_filenames, n_clusters=5)\n",
        "visualize_clusters(test_images, dbscan_labels, test_filenames, n_clusters=5)"
      ],
      "metadata": {
        "id": "ghAyNu3oUmhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3: Dimensionality Reduction and Visualization\n",
        "\n",
        "1. Apply Principal Component Analysis (PCA) and t-SNE to reduce image feature\n",
        "dimensions.\n",
        "2. Visualize the results in 2D plots.\n",
        "\n",
        "## Deliverables:\n",
        "* Code for PCA and t-SNE\n",
        "* Scatter plots with labeled clusters\n",
        "* Discussion on differences in PCA vs. t-SNE results"
      ],
      "metadata": {
        "id": "HASMAF72rag0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hY0LL6dmrlAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4: Contrastive Learning for Representation Learning\n",
        "\n",
        "1. Implement SimCLR or another contrastive learning approach to learn visual\n",
        "representations.\n",
        "2. Train the model using contrastive loss and visualize the learned representations using tSNE.\n",
        "3. Evaluate the learned representations by training a simple classifier on top of them.\n",
        "## Deliverables:\n",
        "* Code for SimCLR implementation\n",
        "* t-SNE visualization of learned representations\n",
        "* Comparison of classifier accuracy with vs. without contrastive learning\n"
      ],
      "metadata": {
        "id": "J7FGgKE0rlVs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QbSaDp0grvnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 5: Evaluation and Comparative Analysis\n",
        "\n",
        "1. Compare the performance of **clustering-based vs. contrastive learning-based**\n",
        "representations.\n",
        "2. Analyze which method is more effective for image categorization and why.\n",
        "3. Provide a **3-4 paragraph report** summarizing your findings, including strengths and\n",
        "limitations of each approach (do this on the Jupyter notebook). The report should include\n",
        "**methodology**, **key observations**, and **performance comparisons**.\n",
        "\n",
        "## Deliverables:\n",
        "* Comparative analysis (tables/graphs)\n",
        "* Report discussing insights and observations"
      ],
      "metadata": {
        "id": "R3O5fiSqrv80"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k1XpDtuYsFd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bonus Task\n",
        "\n",
        "1. Explore a **GAN-based unsupervised learning approach** (e.g., DeepCluster, SelfGAN)\n",
        "for feature learning.\n",
        "2. Compare its performance with SimCLR.\n",
        "\n",
        "## Deliverables:\n",
        "* Code implementation\n",
        "* Brief analysis of results"
      ],
      "metadata": {
        "id": "hyUicXR_sF0T"
      }
    }
  ]
}